{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd4c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 07:50:33.432319: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-28 07:50:33.485400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-28 07:50:34.473429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from convnet import ConvDipNet\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "from CNN_LSTM.util import *\n",
    "from dipoleDataset import DipoleDataset\n",
    "import os\n",
    "from esinet.evaluate import eval_auc, eval_nmse, eval_mse, eval_mean_localization_error\n",
    "import json\n",
    "from util import solve_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c69403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvDipNet                               [32, 5124]                --\n",
       "├─Conv2d: 1-1                            [32, 8, 9, 9]             80\n",
       "├─BatchNorm2d: 1-2                       [32, 8, 9, 9]             16\n",
       "├─Linear: 1-3                            [32, 512]                 332,288\n",
       "├─BatchNorm1d: 1-4                       [32, 512]                 1,024\n",
       "├─Linear: 1-5                            [32, 5124]                2,628,612\n",
       "==========================================================================================\n",
       "Total params: 2,962,020\n",
       "Trainable params: 2,962,020\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 94.99\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1.91\n",
       "Params size (MB): 11.85\n",
       "Estimated Total Size (MB): 13.76\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "in_channels = 1\n",
    "im_shape = (9,9)\n",
    "n_filters = 8\n",
    "kernel_size = (3,3)\n",
    "\n",
    "# create single input ConvDipNet \n",
    "convnet: nn.Module  = ConvDipNet(in_channels, im_shape, n_filters, kernel_size)\n",
    "\n",
    "\n",
    "# print model summary\n",
    "summary(convnet, input_size=(32, 1, im_shape[0], im_shape[1])) # (batch_size, n_timesteps, in_channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a55af6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvDipNet(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (hidden_layer): Linear(in_features=648, out_features=512, bias=True)\n",
       "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (output_layer): Linear(in_features=512, out_features=5124, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"/mnt/data/convdip/model/convdip_run3\"\n",
    "model_weight_path = os.path.join(model_dir, \"convdip__whd100.pt\")\n",
    "convnet.load_state_dict(torch.load(model_weight_path, weights_only=True))\n",
    "convnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728aa07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"/mnt/data/convdip/model/\"\n",
    "loss_save_path = \"/mnt/data/convdip/model/convdip_loss.npy\"\n",
    "data_path = \"/mnt/data/convdip/training_data/\"\n",
    "eeg_data_path = os.path.join(data_path, \"eeg_data\")\n",
    "interp_data_path = os.path.join(data_path, \"interp_data\")\n",
    "source_data_path = os.path.join(data_path, \"source_data\")\n",
    "info_path = os.path.join(data_path, \"info.fif\")\n",
    "\n",
    "dataset = DipoleDataset(eeg_data_path, interp_data_path, source_data_path, info_path, im_shape=im_shape)\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "\n",
    "test_amount, val_amount = int(dataset.__len__() * test_size), int(dataset.__len__() * val_size)\n",
    "\n",
    "# this function will automatically randomly split your dataset but you could also implement the split yourself\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(0) # this is the seed we use to split the data the same way each time\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [\n",
    "            (dataset.__len__() - (test_amount + val_amount)), \n",
    "            test_amount, \n",
    "            val_amount\n",
    "], generator=gen)\n",
    "\n",
    "B = 512  # batch size\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_set,\n",
    "            batch_size=B,\n",
    "            shuffle=False,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "            val_set,\n",
    "            batch_size=B,\n",
    "            shuffle=False,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_set,\n",
    "            batch_size=B,\n",
    "            shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e268634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24175\n",
      "torch.Size([1, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "dipole_pos = np.load(os.path.join(data_path, \"dipole_pos.npy\"))\n",
    "\n",
    "idx, sample, target = val_dataloader.dataset[0]\n",
    "print(idx)\n",
    "print(sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0330d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  80 | elapsed:    5.3s remaining:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  80 | elapsed:    5.5s remaining:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  80 | elapsed:    5.6s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  80 | elapsed:    5.7s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  80 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  80 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  80 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  80 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  80 | elapsed:    1.5s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  80 | elapsed:    1.7s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  80 | elapsed:    1.9s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  80 | elapsed:    2.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "# create forward model\n",
    "fs = 100\n",
    "info = get_info(sfreq=fs)\n",
    "fwd = create_forward_model(sampling='ico4', info=info)\n",
    "leadfield = fwd['sol']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "241a83e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample:   9%|▉         | 46/512 [05:10<52:27,  6.75s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m      mse \u001b[38;5;241m=\u001b[39m eval_mse(target_sample, output_sample)\n\u001b[1;32m     30\u001b[0m      nmse \u001b[38;5;241m=\u001b[39m eval_nmse(target_sample, output_sample)\n\u001b[0;32m---> 31\u001b[0m      metrics_per_sample[data_idx] \u001b[38;5;241m=\u001b[39m [\u001b[43mauc_close\u001b[49m, auc_far, mle, mse, nmse]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# with open(metric_save_path, \"w\") as json_file:\u001b[39;00m\n\u001b[1;32m     35\u001b[0m      \u001b[38;5;66;03m#json.dump(metrics_per_sample, json_file)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/esienv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/esienv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/esienv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/esienv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "metric_save_path = os.path.join(model_dir, \"evaluation_metrics.json\")\n",
    "convnet.to(device)\n",
    "with torch.no_grad():\n",
    "    metrics_per_sample = {}\n",
    "    i=0\n",
    "    for idxs, batch, target in test_dataloader:\n",
    "        i+=1\n",
    "        print(i)\n",
    "        batch = batch.to(device, dtype=torch.float)\n",
    "        output = convnet(batch)\n",
    "        output = output.cpu()\n",
    "\n",
    "        for idx in tqdm(range(output.shape[0]), position=0, desc=\"sample\"):\n",
    "            data_idx = int(idxs[idx])\n",
    "            target_sample = np.array(target[idx])\n",
    "            output_sample = np.array(output[idx])\n",
    "            \n",
    "            eeg = np.load(os.path.join(data_path, f\"eeg_data/sample_{data_idx}.npy\"))\n",
    "            max_idx = np.unravel_index(np.argmax(eeg), eeg.shape)[1] # this is the timestep with the maximum eeg value, this will be used to train\n",
    "            output_sample = solve_p(output_sample, eeg[:,max_idx], leadfield)\n",
    "\n",
    "            \n",
    "            auc_close, auc_far = eval_auc(target_sample, output_sample, dipole_pos)\n",
    "            sample_auc = (auc_close + auc_far)/2\n",
    "            \n",
    "            mle = eval_mean_localization_error(target_sample, output_sample, dipole_pos)\n",
    "            mse = eval_mse(target_sample, output_sample)\n",
    "            nmse = eval_nmse(target_sample, output_sample)\n",
    "            metrics_per_sample[data_idx] = [auc_close, auc_far, mle, mse, nmse]\n",
    "            \n",
    "        \n",
    "       # with open(metric_save_path, \"w\") as json_file:\n",
    "            #json.dump(metrics_per_sample, json_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5abb2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9771': [0.4145663988657845, 0.29337429111531194, 36.312774929301, 0.009217227703145066, 0.13296913452703557], '4374': [0.22827919333413837, 0.08673348629392585, 35.02693422038831, 0.01775956286581762, 0.21168121131165538], '54679': [0.32557755102040814, 0.11291224489795919, nan, 0.014980096009553266, 0.14347386502750029]}\n"
     ]
    }
   ],
   "source": [
    "with open(metric_save_path, 'r') as json_file:\n",
    "    metrics = json.load(json_file)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
